{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gerando Músicas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2O/K0gGD8EnQn45vHvEkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrohortencio/text-generator/blob/main/Song%20Generator/Gerando_M%C3%BAsicas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_Z48Hv_aGV6G"
      },
      "source": [
        "#@title Setup\n",
        "#@markdown _Execute essa célula primeiro. Duplo clique exibe o código._\n",
        " \n",
        "print(\"Iniciando\")\n",
        "\n",
        "TOTAL_WORDS_BASE = 3071\n",
        "MAX_SEQ_LEN_BASE = 83\n",
        "TOTAL_WORDS_ATT = 6760\n",
        "MAX_SEQ_LEN_ATT = 152\n",
        "\n",
        "import numpy as np\n",
        "from IPython.utils import io\n",
        "\n",
        "print(\"Configurando o Keras\")\n",
        "### Reinstalando o Keras\n",
        "with io.capture_output() as captured:\n",
        "    !pip uninstall keras -y\n",
        "    !pip install -U keras\n",
        "\n",
        "### Baixando o arquivo generic_utils.py diretamente do repositório do TensorFlow\n",
        "import os\n",
        "from shutil import copyfile\n",
        "\n",
        "path_utils = '/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py'\n",
        "url_utils = 'https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/python/keras/utils/generic_utils.py'\n",
        "\n",
        "with io.capture_output() as captured:\n",
        "    os.remove(path_utils)\n",
        "    !wget  $url_utils\n",
        "    copyfile('/content/generic_utils.py', path_utils)\n",
        "\n",
        "print(\"Adicionando Attention\")\n",
        "#### Instalando o módulo de self-attention\n",
        "with io.capture_output() as captured:\n",
        "    !pip install keras-self-attention\n",
        "from tensorflow import keras\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "\n",
        "#url = 'https://raw.githubusercontent.com/thushv89/attention_keras/322a16ee147122026b63305aaa5e899d9e5de883/src/layers/attention.py'\n",
        "#!wget $url\n",
        "#from attention import AttentionLayer\n",
        "\n",
        "### Importando bibliotecas\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, BatchNormalization, GlobalMaxPooling1D, GRU, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "### URLs\n",
        "tokenizer_base_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/tokenizer14012021BASE.pickle\"\n",
        "tokenizer_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/tokenizer13012021CONV1D.pickle\"\n",
        "\n",
        "model_base_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/treino-14012021BASE.h5\"\n",
        "model_conv_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/treino-13012021CONV1D.h5\"\n",
        "model_bilstmV2_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/treino-13012021BiLSTM.h5\"\n",
        "model_bilstmV1_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/treino-09062021BiLSTM.h5\"\n",
        "model_bigru_url = \"https://github.com/pedrohortencio/text-generator/raw/main/Song%20Generator/model-files/treino-13012021BiGRU.h5\"\n",
        "\n",
        "\n",
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "print(\"Importando Tokenizers\")\n",
        "\n",
        "tokenizer_base_pth, _ = urllib.request.urlretrieve(tokenizer_base_url, filename=\"/content/tokenizer_base.pickle\")\n",
        "tokenizer_pth, _ = urllib.request.urlretrieve(tokenizer_url, filename=\"/content/tokenizer.pickle\")\n",
        "\n",
        "with open(tokenizer_base_pth, 'rb') as handle:\n",
        "    tokenizer_base = pickle.load(handle)\n",
        "with open(tokenizer_pth, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "print(\"Criando Modelos\")\n",
        "\n",
        "print(\"1/5\")\n",
        "def biLSTMV1():\n",
        "    EMBEDDING_SIZE = 300\n",
        "    LSTM_UNITS = 128\n",
        "    adam = Adam()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(TOTAL_WORDS_ATT, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN_ATT-1, mask_zero=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_width=25,  attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(TOTAL_WORDS_ATT, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_bilstmV1_pth, _ = urllib.request.urlretrieve(model_bilstmV1_url, filename=\"/content/treino-09062021BiLSTM.h5\")\n",
        "biLSTMV1 = biLSTMV1()\n",
        "biLSTMV1.load_weights(model_bilstmV1_pth)\n",
        "\n",
        "print(\"2/5\")\n",
        "def biLSTMV2():\n",
        "    EMBEDDING_SIZE = 300\n",
        "    LSTM_UNITS = 150\n",
        "    adam = Adam()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(TOTAL_WORDS_ATT, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN_ATT-1, mask_zero=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_width=20,  attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(TOTAL_WORDS_ATT, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_bilstmV2_pth, _ = urllib.request.urlretrieve(model_bilstmV2_url, filename=\"/content/treino-13012021BiLSTM.h5\")\n",
        "biLSTMV2 = biLSTMV2()\n",
        "biLSTMV2.load_weights(model_bilstmV2_pth)\n",
        "\n",
        "print(\"3/5\")\n",
        "def biGRU():\n",
        "    EMBEDDING_SIZE = 300\n",
        "    LSTM_UNITS = 150\n",
        "    adam = Adam()\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(TOTAL_WORDS_ATT, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN_ATT-1, mask_zero=True))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Bidirectional(GRU(LSTM_UNITS, return_sequences=True)))\n",
        "    model.add(SeqSelfAttention(attention_width=25,  attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(TOTAL_WORDS_ATT, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_bigru_pth, _ = urllib.request.urlretrieve(model_bigru_url, filename=\"/content/treino-13012021BiGRU.h5\")\n",
        "biGRU = biGRU()\n",
        "biGRU.load_weights(model_bigru_pth)\n",
        "\n",
        "print(\"4/5\")\n",
        "def m_conv1D():\n",
        "    EMBEDDING_SIZE = 300\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(TOTAL_WORDS_ATT, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN_ATT-1, mask_zero=True))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv1D(512, 10, padding='same', activation='relu'))\n",
        "    model.add(SeqSelfAttention(attention_width=25, attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv1D(256, 5, padding='same', activation='relu'))\n",
        "    model.add(SeqSelfAttention(attention_width=15, attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation='sigmoid'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(TOTAL_WORDS_ATT, activation='softmax'))\n",
        "\n",
        "    adam = Adam()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_conv_pth, _ = urllib.request.urlretrieve(model_conv_url, filename=\"/content/treino-13012021CONV1D.h5\")\n",
        "m_conv1D = m_conv1D()\n",
        "m_conv1D.load_weights(model_conv_pth)\n",
        "\n",
        "print(\"5/5\")\n",
        "def base_model():\n",
        "    EMBEDDING_SIZE = 300\n",
        "    LSTM_UNITS = 256\n",
        "    LEARNING_RATE = 0.01\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(TOTAL_WORDS_BASE, EMBEDDING_SIZE, input_length=MAX_SEQ_LEN_BASE-1))\n",
        "    model.add(Bidirectional(LSTM(LSTM_UNITS)))\n",
        "    model.add(Dense(TOTAL_WORDS_BASE, activation='softmax'))\n",
        "\n",
        "    adam = Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model_base_pth, _ = urllib.request.urlretrieve(model_base_url, filename=\"/content/treino-14012021BASE.h5\")\n",
        "base_model = base_model()\n",
        "base_model.load_weights(model_base_pth)\n",
        "\n",
        "\n",
        "\n",
        "def predict_base(seed_text, model, next_words=50):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer_base.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=MAX_SEQ_LEN_BASE-1, padding='pre')\n",
        "        #predicted = model.predict_classes(token_list, verbose=0)\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer_base.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    \n",
        "    versos = seed_text.split(' <br> ')\n",
        "    counter = -1\n",
        "    value = 4\n",
        "    print('\\n Musica: ',versos[0],'\\n')\n",
        "    for v in versos[1:]:\n",
        "        print(v.capitalize())\n",
        "        counter = counter + 1\n",
        "        if counter == value:\n",
        "            print('\\n')\n",
        "            counter = 0\n",
        "\n",
        "def predict_att(seed_text, model, next_words=50):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=MAX_SEQ_LEN_ATT-1, padding='pre')\n",
        "        #predicted = model.predict_classes(token_list, verbose=0)\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    \n",
        "    versos = seed_text.split(' <br> ')\n",
        "    counter = -1\n",
        "    value = 4\n",
        "    print('\\n Musica: ',versos[0],'\\n')\n",
        "    for v in versos[1:]:\n",
        "        print(v.capitalize())\n",
        "        counter = counter + 1\n",
        "        if counter == value:\n",
        "            print('\\n')\n",
        "            counter = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rssg1LE2glBo"
      },
      "source": [
        ">Sugestões de frases (Por favor, digite sem aspas):\n",
        ">\n",
        ">* \"Você é minha vida\"\n",
        "* \"Amor da minha vida\"\n",
        "* \"Já vi que você vai sair \\<br> E a sequência eu já sei\"\n",
        "* \"Ele não faz questão de pegar na sua mão \\<br> Eu nunca vi ele te dar beijo na frente de ninguém \\<br> Tem algo errado nessa relação e você sabe quem\"\n",
        "* \"Barzinho outra vez\"\n",
        "* \"Disk-recaída, bateu carência liga\"\n",
        "* \"Agora não chega mais nenhuma notificação sua\"\n",
        "* \"Você não é nada além de uma causa perdida\"\n",
        "* \"Vou tentar te esquecer\"\n",
        "* \"Seguir seus passos já não posso mais\"\n",
        "* \"Nunca me enxerguei em outra pessoa\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ePMcIMdRYtpD"
      },
      "source": [
        "#@title Geração de Músicas\n",
        "#@markdown _Execute quantas vezes quiser. Note que a execução pode demorar alguns minutos._\n",
        "seed = input(\"Digite uma frase que servirá como o título da música gerada:\\n\")\n",
        "qtd_palavras = int(input(\"Quantas palavras por música (default=50):\\n\"))\n",
        "\n",
        "lista_modelos = [biLSTMV1, biLSTMV2, biGRU, m_conv1D, base_model]\n",
        "desc_modelos = [\"Bidirectional LSTM + Attention (V1)\", \"Bidirectional LSTM + Attention (V2)\",\n",
        "                \"Bidirectional GRU + Attention\", \"Conv1D + Attention\", \"Bidirectional LSTM (Base)\"]\n",
        "\n",
        "for model, desc in zip(lista_modelos, desc_modelos):\n",
        "    print(\"\\n\", \"--\"*25, )\n",
        "    print(\"Modelo: \", desc, \"\\n\")\n",
        "\n",
        "    if model != base_model:\n",
        "        predict_att(seed, model, qtd_palavras)\n",
        "    else:\n",
        "        predict_base(seed, model, qtd_palavras)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxpqpGGodDMK"
      },
      "source": [
        "## Sobre os Modelos\n",
        "\n",
        "O código fonte de criação dos modelos e seus respectivos pesos estão disponíveis no [GitHub](https://github.com/pedrohortencio/text-generator/tree/main/Song%20Generator).\n",
        "\n",
        "São 5 modelos apresentados aqui, ao todo:\n",
        "* Bidirectional LSTM (Base)\n",
        "> Um modelo com um único layer LSTM bidirecional. Gera bons resultados, mas tem a tendência a repetir frases e palavras quando o número de palavras geradas é alto. Há, também, a tendência a simplesmente copiar trechos de músicas do conjunto de treinamento.\n",
        "\n",
        "* Bidirectional LSTM + Attention (V1)\n",
        "> É a implementação de self-attention no modelo anterior. A adição de self-attention visa resolver o problema de cópia e repetição de palavras.\n",
        "\n",
        "\n",
        "* Bidirectional LSTM + Attention (V2)\n",
        "> Pequenas modificações no modelo anterior. Conta com 150 unidades (em vez de 128) e foi treinado por uma maior quantidade de épocas.\n",
        "\n",
        "* Bidirectional GRU + Attention\n",
        "> Implementação que substitui o layer LSTM do modelo anterior por um layer GRU.\n",
        "\n",
        "* Conv1D + Attention\n",
        "> Assim como o modelo com GRU, essa implementação substitui a camada LSTM por duas camadas Conv1D, com a operação MaxPooling1D entre elas. Ambas as camadas possuem self-attention. Foi o modelo com o menor tempo de treinamento (de longe o menor, consumindo 40 minutos contra 1h-2h dos modelos anteriores)."
      ]
    }
  ]
}